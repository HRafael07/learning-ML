import numpy as np 
import matplotlib.pyplot as plt 
from numpy import ndarray 
from typing import Dict 
from sklearn import datasets 

#diabetes = datasets.load_diabetes() 
#x = diabetes.data 
#y = diabetes.target 

x = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]])
y = np.array([[6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84]])

def initial_weights(): 
    weights: Dict[str, ndarray] = {}
    weights['W'] = np.random.randn(x.shape[1], 1)
    weights['B'] = np.random.randn(1,1)
    return weights

def loss_function(weights, x, y): 
    p = np.dot(x, weights['W']) + weights['B']
    loss_total = np.mean((y - p) ** 2) 
    return loss_total, p

def gradient_descendent(x, y, p): 
    dLdW = (1/x.shape[1]) * np.dot(x.T, (p - y))
    dLdB = (1/x.shape[1]) * np.sum((p - y))
    gradient: Dict[str, ndarray] = {} 
    gradient['W'] = dLdW 
    gradient['B'] = dLdB 
    return gradient 

def train(epochs: int = 5000, lr: float = 0.001): 
    losses = [] 
    weights = initial_weights()
    for i in range (epochs): 
        loss_total, p = loss_function(weights, x, y) 
        losses.append(loss_total)
        gradient = gradient_descendent(x, y, p)
        for key in weights.keys():
            weights[key] = weights[key] - lr * gradient[key]
    
    return losses, weights

losses, weights = train() 

def predict(x): 
    y_pred = np.dot(x, weights['W']) + weights['B']  
    return y_pred
    
y_predline = predict(x).reshape(40,)
print(y_predline)
plt.scatter(x, y, label='Dados')
plt.plot(x[0], y_predline, color = 'red')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Regress√£o Linear Simples')
plt.show()
